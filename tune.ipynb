{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0615169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jchang153/miniforge3/envs/tf-metal/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, enum, time, os, random, math\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn as nn\n",
    "from mask import *\n",
    "from process_data import *\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db4244",
   "metadata": {},
   "source": [
    "## Loading in and inspecting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c334b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jchang153/miniforge3/envs/tf-metal/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/jchang153/miniforge3/envs/tf-metal/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <3F789787-FE38-3CE7-8599-064BDD0416EE> /Users/jchang153/miniforge3/envs/tf-metal/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <6B8AC17B-04CC-36D0-BD01-780381EFB0CC> /Users/jchang153/miniforge3/envs/tf-metal/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ffef75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072d4ac",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5aed3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " so you have this thing that shows what it does.\n",
      "\n",
      "$ fibonacci 1\n",
      "\n",
      "There are four fibonacci blocks (which are also called the N-dimensional array) that we know to be N-dimensional: the integer, the square root, and the nonce. The third form of the N-dimensional array is the diagonal of the array. If you multiply the array by two, then it will represent the diagonal of the entire N-dimensional array (in other words\n",
      "---\n",
      "Full output:\n",
      "Help me code up the fibonacci sequence in Python so you have this thing that shows what it does.\n",
      "\n",
      "$ fibonacci 1\n",
      "\n",
      "There are four fibonacci blocks (which are also called the N-dimensional array) that we know to be N-dimensional: the integer, the square root, and the nonce. The third form of the N-dimensional array is the diagonal of the array. If you multiply the array by two, then it will represent the diagonal of the entire N-dimensional array (in other words\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Help me code up the fibonacci sequence in Python\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "max_new_tokens = 100\n",
    "temperature = 1.0\n",
    "top_k = 50\n",
    "\n",
    "generated = input_ids.clone()\n",
    "\n",
    "# Step-by-step autoregressive loop\n",
    "for step in range(max_new_tokens):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(generated)\n",
    "        # model outputs logits for each token in the input sequence; only get last token's logits\n",
    "        next_token_logits = outputs.logits[:, -1, :] / temperature \n",
    "\n",
    "        # Top-k sampling\n",
    "        topk_logits, topk_indices = torch.topk(next_token_logits, top_k)\n",
    "        probs = torch.softmax(topk_logits, dim=-1)\n",
    "        next_token = topk_indices[0][torch.multinomial(probs, num_samples=1)]\n",
    "\n",
    "    # append new token\n",
    "    generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "    # decode just the new token\n",
    "    new_text = tokenizer.decode(next_token[0])\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n---\\nFull output:\")\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ff518",
   "metadata": {},
   "source": [
    "## Registering hooks & getting masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19116235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register hooks with split fraction a = 0.5\n",
    "active, masks_1, masks_2 = register_hooks(model, a=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e4ee3",
   "metadata": {},
   "source": [
    "## Loading in txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"OpenCoder-LLM/opc-sft-stage2\", \"educational_instruct\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a84cdd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = ds.remove_columns([c for c in ds.column_names if c != \"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc975859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36322229"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = \"\"\n",
    "for i in range(len(text_ds)):\n",
    "    data1 += text_ds[i]['code']\n",
    "len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc9c01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1[:5337651]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34273d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample_code.txt\", \"w\") as f:\n",
    "    f.write(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8d6ba67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5337651"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/shakespeare.txt\", \"r\") as f:\n",
    "    data2 = f.read()\n",
    "len(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5994af",
   "metadata": {},
   "source": [
    "## Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8798ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = model.config.n_ctx\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04dfc573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1809728 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "paths = ['data/shakespeare.txt']\n",
    "\n",
    "train_ds = SimpleTextDataset(paths, tokenizer, context, DatasetSplit.train)\n",
    "val_ds   = SimpleTextDataset(paths, tokenizer, context, DatasetSplit.valid)\n",
    "test_ds  = SimpleTextDataset(paths, tokenizer, context, DatasetSplit.test)\n",
    "\n",
    "train_loader_1 = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader_1   = DataLoader(val_ds, batch_size=batch_size)\n",
    "test_loader_1  = DataLoader(test_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e381f738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader_1), len(val_loader_1), len(test_loader_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1149c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['data/code.txt']\n",
    "\n",
    "train_ds = SimpleTextDataset(paths, tokenizer, context, DatasetSplit.train)\n",
    "val_ds   = SimpleTextDataset(paths, tokenizer, context, DatasetSplit.valid)\n",
    "test_ds  = SimpleTextDataset(paths, tokenizer, context, DatasetSplit.test)\n",
    "\n",
    "train_loader_2 = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader_2   = DataLoader(val_ds, batch_size=batch_size)\n",
    "test_loader_2  = DataLoader(test_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0176ce82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 5, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader_2), len(val_loader_2), len(test_loader_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36f116",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eda89baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, active_masks=None, max_batches=None):\n",
    "    if active_masks is not None:\n",
    "        active.update(active_masks)   # switch masks for this eval\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (x, y) in enumerate(loader, start=1):\n",
    "            x, y = x.to(device), y.to(device)     # [B, T]\n",
    "            out = model(input_ids=x, labels=y)    # GPT-2 computes CE internally\n",
    "            # weight by number of supervised tokens (B*T)\n",
    "            total_loss += out.loss.item() * x.numel()\n",
    "            total_tokens += x.numel()\n",
    "\n",
    "            if max_batches is not None and step >= max_batches:\n",
    "                break\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / max(1, total_tokens)   # per-token average loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce13b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graceful Exit\n"
     ]
    }
   ],
   "source": [
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "CKPT_DIR = f\"checkpoints/{ts}/\"\n",
    "LOG_PATH = CKPT_DIR + \"log.txt\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# write header once if log doesn't exist\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    with open(LOG_PATH, \"w\") as f:\n",
    "        f.write(\"epoch,elapsed_sec,train_loss1,train_ppl1,train_loss2,train_ppl2,\"\n",
    "                \"val_loss1,val_ppl1,val_loss2,val_ppl2\\n\")\n",
    "\n",
    "lr = 3.5e-4\n",
    "clip = 0.25\n",
    "best_val_loss_1 = None\n",
    "best_val_loss_2 = None\n",
    "epochs = 1\n",
    "log_interval = 50\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        val_loss_1 = evaluate(val_loader_1, active_masks=masks_1, max_batches=3)\n",
    "        val_loss_2 = evaluate(val_loader_2, active_masks=masks_2, max_batches=3)\n",
    "\n",
    "        print('-' * 100)\n",
    "        print('| checkpoint | epoch {:3d} | time: {:5.2f}s | '\n",
    "              'validation loss 1 {:5.3f} | validation ppl 1 {:8.2f} | '\n",
    "              'validation loss 2 {:5.3f} | validation ppl 2 {:8.2f}'\n",
    "              .format(epoch, (time.time() - t0),\n",
    "                      val_loss_1, math.exp(val_loss_1),\n",
    "                      val_loss_2, math.exp(val_loss_2)))\n",
    "        print('-' * 100)\n",
    "        print('epoch\\tstep%\\tms/batch\\tlr\\tloss1\\tppl1\\tloss2\\tppl2')\n",
    "\n",
    "        if best_val_loss_1 is None or val_loss_1 < best_val_loss_1:\n",
    "            best_val_loss_1 = val_loss_1\n",
    "            torch.save({...}, os.path.join(CKPT_DIR, \"best_1.pt\"))\n",
    "        if best_val_loss_2 is None or val_loss_2 < best_val_loss_2:\n",
    "            best_val_loss_2 = val_loss_2\n",
    "            torch.save({...}, os.path.join(CKPT_DIR, \"best_2.pt\"))\n",
    "\n",
    "        # ---------------- training ----------------\n",
    "        model.train()\n",
    "        running_loss1 = 0.0\n",
    "        running_loss2 = 0.0\n",
    "        t_batch = time.time()\n",
    "\n",
    "        # epoch-level (token-weighted) accumulators for train stats\n",
    "        train_loss1_tokensum = 0.0\n",
    "        train_loss2_tokensum = 0.0\n",
    "        train_tokens1 = 0\n",
    "        train_tokens2 = 0\n",
    "\n",
    "        for step, ((x1, y1), (x2, y2)) in enumerate(zip(train_loader_1, train_loader_2), start=1):\n",
    "            x1, y1 = x1.to(device), y1.to(device)  # [B, T]\n",
    "            x2, y2 = x2.to(device), y2.to(device)  # [B, T]\n",
    "\n",
    "            # set L1 masks active; compute grads on L1 parameters\n",
    "            active.update(masks_1)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss1 = model(input_ids=x1, labels=y1).loss\n",
    "            loss1.backward()\n",
    "\n",
    "            # set L2 masks active; compute grads on L2 parameters\n",
    "            active.update(masks_2)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss2 = model(input_ids=x2, labels=y2).loss\n",
    "            loss2.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # running display\n",
    "            running_loss1 += loss1.item()\n",
    "            running_loss2 += loss2.item()\n",
    "\n",
    "            # epoch-level token-weighted stats\n",
    "            train_loss1_tokensum += loss1.item() * x1.numel()\n",
    "            train_tokens1 += x1.numel()\n",
    "            train_loss2_tokensum += loss2.item() * x2.numel()\n",
    "            train_tokens2 += x2.numel()\n",
    "\n",
    "            if (step % log_interval) == 0:\n",
    "                elapsed_ms = (time.time() - t_batch) * 1000.0 / log_interval\n",
    "                avg1 = running_loss1 / log_interval\n",
    "                avg2 = running_loss2 / log_interval\n",
    "                pct = 100.0 * step / float(len(train_loader_1))  # zip() â†’ shorter loader length\n",
    "                print(f'{epoch:3d}\\t{pct:4.1f}%\\t{elapsed_ms:7.2f}\\t{lr:.3g}\\t'\n",
    "                      f'{avg1:6.3f}\\t{math.exp(avg1):7.2f}\\t{avg2:6.3f}\\t{math.exp(avg2):7.2f}')\n",
    "                running_loss1 = 0.0\n",
    "                running_loss2 = 0.0\n",
    "                t_batch = time.time()\n",
    "\n",
    "        # ---------------- epoch end: compute epoch-level train stats ----------------\n",
    "        epoch_train_loss1 = train_loss1_tokensum / max(1, train_tokens1)\n",
    "        epoch_train_loss2 = train_loss2_tokensum / max(1, train_tokens2)\n",
    "        epoch_train_ppl1 = math.exp(epoch_train_loss1)\n",
    "        epoch_train_ppl2 = math.exp(epoch_train_loss2)\n",
    "\n",
    "        # ---------------- always-save checkpoint for this epoch ----------------\n",
    "        ckpt_path = os.path.join(CKPT_DIR, f\"epoch_{epoch:03d}.pt\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"val_loss_1\": val_loss_1,\n",
    "            \"val_loss_2\": val_loss_2,\n",
    "            \"train_loss_1\": epoch_train_loss1,\n",
    "            \"train_loss_2\": epoch_train_loss2,\n",
    "        }, ckpt_path)\n",
    "\n",
    "        # also keep a convenient \"latest.pt\"\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"val_loss_1\": val_loss_1,\n",
    "            \"val_loss_2\": val_loss_2,\n",
    "            \"train_loss_1\": epoch_train_loss1,\n",
    "            \"train_loss_2\": epoch_train_loss2,\n",
    "        }, os.path.join(CKPT_DIR, \"latest.pt\"))\n",
    "\n",
    "        # ---------------- append to log.txt ----------------\n",
    "        elapsed_sec = time.time() - start_time\n",
    "        with open(LOG_PATH, \"a\") as f:\n",
    "            f.write(f\"{epoch},{elapsed_sec:.2f},\"\n",
    "                    f\"{epoch_train_loss1:.6f},{epoch_train_ppl1:.4f},\"\n",
    "                    f\"{epoch_train_loss2:.6f},{epoch_train_ppl2:.4f},\"\n",
    "                    f\"{val_loss_1:.6f},{math.exp(val_loss_1):.4f},\"\n",
    "                    f\"{val_loss_2:.6f},{math.exp(val_loss_2):.4f}\\n\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Graceful Exit')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
