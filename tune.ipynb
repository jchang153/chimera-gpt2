{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5725c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (4.57.1)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (4.4.0)\n",
      "Requirement already satisfied: hf_transfer in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (0.1.9)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in ./.venv/lib/python3.13/site-packages (from torch->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.13/site-packages (from transformers->-r requirements.txt (line 3)) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from transformers->-r requirements.txt (line 3)) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers->-r requirements.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers->-r requirements.txt (line 3)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers->-r requirements.txt (line 3)) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers->-r requirements.txt (line 3)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.13/site-packages (from transformers->-r requirements.txt (line 3)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers->-r requirements.txt (line 3)) (0.6.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.13/site-packages (from datasets->-r requirements.txt (line 4)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from datasets->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from datasets->-r requirements.txt (line 4)) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.13/site-packages (from datasets->-r requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.13/site-packages (from datasets->-r requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.13/site-packages (from datasets->-r requirements.txt (line 4)) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (3.13.2)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 4)) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 4)) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 4)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 4)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements.txt (line 4)) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 4)) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0615169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jchang153/miniforge3/envs/tf-metal/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, enum, time, os, random, math\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn as nn\n",
    "from mask import *\n",
    "from process_data import *\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db4244",
   "metadata": {},
   "source": [
    "## Loading in and inspecting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c334b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jchang153/miniforge3/envs/tf-metal/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/jchang153/miniforge3/envs/tf-metal/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <3F789787-FE38-3CE7-8599-064BDD0416EE> /Users/jchang153/miniforge3/envs/tf-metal/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <6B8AC17B-04CC-36D0-BD01-780381EFB0CC> /Users/jchang153/miniforge3/envs/tf-metal/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ffef75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072d4ac",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5aed3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def fibonacci(n, x): self.i = x\n",
      "\n",
      "return 10\n",
      "\n",
      "def lerp(n, x: Int): return 2+'1\n",
      "\n",
      "return 30\n",
      "\n",
      "def fibonacci(n, x: Color): self.x = x\n",
      "\n",
      "return 12\n",
      "\n",
      "def fibonacci4(n, x: Int): self.x1 = x\n",
      "\n",
      "return 30\n",
      "\n",
      "def fibonacci3(n, x:\n",
      "---\n",
      "Full output:\n",
      "Help me code up the fibonacci sequence in Python\n",
      "\n",
      "def fibonacci(n, x): self.i = x\n",
      "\n",
      "return 10\n",
      "\n",
      "def lerp(n, x: Int): return 2+'1\n",
      "\n",
      "return 30\n",
      "\n",
      "def fibonacci(n, x: Color): self.x = x\n",
      "\n",
      "return 12\n",
      "\n",
      "def fibonacci4(n, x: Int): self.x1 = x\n",
      "\n",
      "return 30\n",
      "\n",
      "def fibonacci3(n, x:\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Help me code up the fibonacci sequence in Python\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "max_new_tokens = 100\n",
    "temperature = 1.0\n",
    "top_k = 50\n",
    "\n",
    "generated = input_ids.clone()\n",
    "\n",
    "# Step-by-step autoregressive loop\n",
    "for step in range(max_new_tokens):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(generated)\n",
    "        # model outputs logits for each token in the input sequence; only get last token's logits\n",
    "        next_token_logits = outputs.logits[:, -1, :] / temperature \n",
    "\n",
    "        # Top-k sampling\n",
    "        topk_logits, topk_indices = torch.topk(next_token_logits, top_k)\n",
    "        probs = torch.softmax(topk_logits, dim=-1)\n",
    "        next_token = topk_indices[0][torch.multinomial(probs, num_samples=1)]\n",
    "\n",
    "    # append new token\n",
    "    generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "    # decode just the new token\n",
    "    new_text = tokenizer.decode(next_token[0])\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n---\\nFull output:\")\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ff518",
   "metadata": {},
   "source": [
    "## Registering hooks & getting masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19116235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register hooks with split fraction a = 0.5\n",
    "active, masks_1, masks_2 = register_hooks(model, a=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e4ee3",
   "metadata": {},
   "source": [
    "## Loading in txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"OpenCoder-LLM/opc-sft-stage2\", \"educational_instruct\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a84cdd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = ds.remove_columns([c for c in ds.column_names if c != \"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc975859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36322229"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = \"\"\n",
    "for i in range(len(text_ds)):\n",
    "    data1 += text_ds[i]['code']\n",
    "len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc9c01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1[:5337651]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34273d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample_code.txt\", \"w\") as f:\n",
    "    f.write(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8d6ba67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5337651"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/shakespeare.txt\", \"r\") as f:\n",
    "    data2 = f.read()\n",
    "len(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5994af",
   "metadata": {},
   "source": [
    "## Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8798ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = model.config.n_ctx\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04dfc573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1809728 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "paths = ['data/shakespeare.txt']\n",
    "\n",
    "train_ds = SimpleTextDataset(paths, tokenizer, context, DatasetSplit.train)\n",
    "val_ds   = SimpleTextDataset(paths, tokenizer, context, DatasetSplit.valid)\n",
    "test_ds  = SimpleTextDataset(paths, tokenizer, context, DatasetSplit.test)\n",
    "\n",
    "train_loader_1 = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader_1   = DataLoader(val_ds, batch_size=batch_size)\n",
    "test_loader_1  = DataLoader(test_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e381f738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 11, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader_1), len(val_loader_1), len(test_loader_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1149c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['data/code.txt']\n",
    "\n",
    "train_ds = SimpleTextDataset(paths, tokenizer, context, DatasetSplit.train)\n",
    "val_ds   = SimpleTextDataset(paths, tokenizer, context, DatasetSplit.valid)\n",
    "test_ds  = SimpleTextDataset(paths, tokenizer, context, DatasetSplit.test)\n",
    "\n",
    "train_loader_2 = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader_2   = DataLoader(val_ds, batch_size=batch_size)\n",
    "test_loader_2  = DataLoader(test_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0176ce82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294, 17, 17)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader_2), len(val_loader_2), len(test_loader_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36f116",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eda89baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, active_masks=None, max_batches=None):\n",
    "    if active_masks is not None:\n",
    "        active.update(active_masks)   # switch masks for this eval\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (x, y) in enumerate(loader, start=1):\n",
    "            x, y = x.to(device), y.to(device)     # [B, T]\n",
    "            out = model(input_ids=x, labels=y)    # GPT-2 computes CE internally\n",
    "            # weight by number of supervised tokens (B*T)\n",
    "            total_loss += out.loss.item() * x.numel()\n",
    "            total_tokens += x.numel()\n",
    "\n",
    "            if max_batches is not None and step >= max_batches:\n",
    "                break\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / max(1, total_tokens)   # per-token average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "495ab206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graceful Exit\n"
     ]
    }
   ],
   "source": [
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "CKPT_DIR = f\"checkpoints/{ts}/\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "LOG_EPOCH = os.path.join(CKPT_DIR, \"log.csv\")\n",
    "LOG_STEP  = os.path.join(CKPT_DIR, \"steps.csv\")\n",
    "\n",
    "if not os.path.exists(LOG_EPOCH):\n",
    "    with open(LOG_EPOCH, \"w\") as f:\n",
    "        f.write(\"epoch,elapsed_sec,train_loss1,train_ppl1,train_loss2,train_ppl2,val_loss1,val_ppl1,val_loss2,val_ppl2\\n\")\n",
    "if not os.path.exists(LOG_STEP):\n",
    "    with open(LOG_STEP, \"w\") as f:\n",
    "        f.write(\"global_step,epoch,step_in_epoch,ms_per_batch,lr,avg_loss1,avg_loss2\\n\")\n",
    "\n",
    "lr = 3.5e-4\n",
    "clip = 0.25\n",
    "epochs = 5\n",
    "log_interval = 1\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_val_loss_1 = None\n",
    "best_val_loss_2 = None\n",
    "start_time = time.time()\n",
    "global_step = 0\n",
    "\n",
    "def _effective_len(loader1, loader2):\n",
    "    return min(len(loader1), len(loader2))\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\n===== Epoch {epoch} =====\")\n",
    "        model.train()\n",
    "        running_loss1 = 0.0\n",
    "        running_loss2 = 0.0\n",
    "        t_batch = time.time()\n",
    "\n",
    "        # epoch-level (token-weighted) accumulators\n",
    "        train_loss1_tokensum = 0.0\n",
    "        train_loss2_tokensum = 0.0\n",
    "        train_tokens1 = 0\n",
    "        train_tokens2 = 0\n",
    "\n",
    "        eff_train_len = _effective_len(train_loader_1, train_loader_2)\n",
    "\n",
    "        for step, ((x1, y1), (x2, y2)) in enumerate(zip(train_loader_1, train_loader_2), start=1):\n",
    "            x1, y1 = x1.to(device), y1.to(device)\n",
    "            x2, y2 = x2.to(device), y2.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # pass 1 (dataset 1)\n",
    "            active.update(masks_1)\n",
    "            loss1 = model(input_ids=x1, labels=y1).loss\n",
    "            loss1.backward()\n",
    "\n",
    "            # pass 2 (dataset 2)\n",
    "            active.update(masks_2)\n",
    "            loss2 = model(input_ids=x2, labels=y2).loss\n",
    "            loss2.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # running stats\n",
    "            l1 = loss1.item()\n",
    "            l2 = loss2.item()\n",
    "            running_loss1 += l1\n",
    "            running_loss2 += l2\n",
    "\n",
    "            # token-weighted epoch aggregates\n",
    "            train_loss1_tokensum += l1 * x1.numel()\n",
    "            train_tokens1 += x1.numel()\n",
    "            train_loss2_tokensum += l2 * x2.numel()\n",
    "            train_tokens2 += x2.numel()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            if (step % log_interval) == 0:\n",
    "                elapsed_ms = (time.time() - t_batch) * 1000.0 / log_interval\n",
    "                avg1 = running_loss1 / log_interval\n",
    "                avg2 = running_loss2 / log_interval\n",
    "                pct = 100.0 * step / float(eff_train_len)\n",
    "\n",
    "                print(f'{epoch:3d}\\t{pct:5.1f}%\\t{elapsed_ms:8.2f}\\t{lr:.3g}\\t'\n",
    "                      f'{avg1:9.5f}\\t{math.exp(avg1):8.2f}\\t{avg2:9.5f}\\t{math.exp(avg2):8.2f}')\n",
    "\n",
    "                with open(LOG_STEP, \"a\") as f:\n",
    "                    f.write(f\"{global_step},{epoch},{step},{elapsed_ms:.3f},{lr:.6g},\"\n",
    "                            f\"{avg1:.6f},{avg2:.6f}\\n\")\n",
    "\n",
    "                running_loss1 = 0.0\n",
    "                running_loss2 = 0.0\n",
    "                t_batch = time.time()\n",
    "\n",
    "        # ---------- epoch end aggregates ----------\n",
    "        epoch_train_loss1 = train_loss1_tokensum / max(1, train_tokens1)\n",
    "        epoch_train_loss2 = train_loss2_tokensum / max(1, train_tokens2)\n",
    "        epoch_train_ppl1 = math.exp(epoch_train_loss1)\n",
    "        epoch_train_ppl2 = math.exp(epoch_train_loss2)\n",
    "\n",
    "        # ---------- evaluation AFTER training ----------\n",
    "        t0 = time.time()\n",
    "        eff_val_len = _effective_len(val_loader_1, val_loader_2)\n",
    "        val_loss_1 = evaluate(val_loader_1, active_masks=masks_1, max_batches=eff_val_len)\n",
    "        val_loss_2 = evaluate(val_loader_2, active_masks=masks_2, max_batches=eff_val_len)\n",
    "        eval_time = time.time() - t0\n",
    "\n",
    "        print('-' * 110)\n",
    "        print(f'| epoch: {epoch:3d} | eval_time: {eval_time:6.2f}s | '\n",
    "              f'val1: {val_loss_1:6.3f} (ppl: {math.exp(val_loss_1):8.2f}) | '\n",
    "              f'val2: {val_loss_2:6.3f} (ppl: {math.exp(val_loss_2):8.2f})')\n",
    "        print('-' * 110)\n",
    "\n",
    "        # ---------- checkpointing ----------\n",
    "        # always save latest\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"val_loss_1\": val_loss_1,\n",
    "            \"val_loss_2\": val_loss_2,\n",
    "            \"train_loss_1\": epoch_train_loss1,\n",
    "            \"train_loss_2\": epoch_train_loss2,\n",
    "        }, os.path.join(CKPT_DIR, \"latest.pt\"))\n",
    "\n",
    "        # best checkpoints\n",
    "        if best_val_loss_1 is None or val_loss_1 < best_val_loss_1:\n",
    "            best_val_loss_1 = val_loss_1\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"which\": \"best_1\",\n",
    "                \"val_loss_1\": val_loss_1,\n",
    "                \"val_loss_2\": val_loss_2,\n",
    "            }, os.path.join(CKPT_DIR, \"best_1.pt\"))\n",
    "\n",
    "        if best_val_loss_2 is None or val_loss_2 < best_val_loss_2:\n",
    "            best_val_loss_2 = val_loss_2\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"which\": \"best_2\",\n",
    "                \"val_loss_1\": val_loss_1,\n",
    "                \"val_loss_2\": val_loss_2,\n",
    "            }, os.path.join(CKPT_DIR, \"best_2.pt\"))\n",
    "\n",
    "        # ---------- epoch log row ----------\n",
    "        elapsed_sec = time.time() - start_time\n",
    "        with open(LOG_EPOCH, \"a\") as f:\n",
    "            f.write(f\"{epoch},{elapsed_sec:.2f},\"\n",
    "                    f\"{epoch_train_loss1:.6f},{epoch_train_ppl1:.4f},\"\n",
    "                    f\"{epoch_train_loss2:.6f},{epoch_train_ppl2:.4f},\"\n",
    "                    f\"{val_loss_1:.6f},{math.exp(val_loss_1):.4f},\"\n",
    "                    f\"{val_loss_2:.6f},{math.exp(val_loss_2):.4f}\\n\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Graceful Exit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c03fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d315eb1",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(CKPT_DIR, \"latest.pt\")  # <--- set this to whichever you want\n",
    "\n",
    "# Load checkpoint safely\n",
    "print(f\"\\n[TEST] Loading checkpoint from: {ckpt_path}\")\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "# Handle both raw state_dict or wrapped payloads\n",
    "state_dict = ckpt if isinstance(ckpt, dict) and all(k.startswith((\"transformer\", \"wte\", \"lm_head\")) for k in ckpt.keys()) \\\n",
    "    else ckpt.get(\"model_state\", ckpt)\n",
    "\n",
    "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "if missing or unexpected:\n",
    "    print(f\"[TEST] load_state_dict: missing={len(missing)}, unexpected={len(unexpected)}\")\n",
    "    if len(missing) < 20:\n",
    "        print(\"  missing:\", missing)\n",
    "    if len(unexpected) < 20:\n",
    "        print(\"  unexpected:\", unexpected)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _effective_len(loader1, loader2):\n",
    "    return min(len(loader1), len(loader2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    eff_test_len = _effective_len(test_loader_1, test_loader_2)\n",
    "\n",
    "    # Objective 1\n",
    "    active.update(masks_1)\n",
    "    test_loss_1 = evaluate(test_loader_1, active_masks=masks_1, max_batches=eff_test_len)\n",
    "    test_ppl_1  = math.exp(test_loss_1)\n",
    "\n",
    "    # Objective 2\n",
    "    active.update(masks_2)\n",
    "    test_loss_2 = evaluate(test_loader_2, active_masks=masks_2, max_batches=eff_test_len)\n",
    "    test_ppl_2  = math.exp(test_loss_2)\n",
    "\n",
    "print(\"-\" * 110)\n",
    "print(f\"| TEST @ {os.path.basename(ckpt_path)} | \"\n",
    "      f\"loss1: {test_loss_1:6.4f} (ppl {test_ppl_1:8.2f}) | \"\n",
    "      f\"loss2: {test_loss_2:6.4f} (ppl {test_ppl_2:8.2f})\")\n",
    "print(\"-\" * 110)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
